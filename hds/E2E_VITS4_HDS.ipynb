{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdd8cf1-70df-4961-afed-844af8813a41",
   "metadata": {},
   "source": [
    "# Imports & preparatory steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8e53c2-dbd2-40b3-aa5f-76fe1d366f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Number of CPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Check the number of CPUs\n",
    "# $PBS_NUM_PPN vs $OMP_NUM_THREADS?\n",
    "N_CPUS = int(os.environ[\"PBS_NUM_PPN\"])\n",
    "print(f\"> Number of CPUs: {N_CPUS}\")\n",
    "\n",
    "# Limit CPU operation in pytorch to `N_CPUS`\n",
    "\n",
    "torch.set_num_threads(N_CPUS)\n",
    "torch.set_num_interop_threads(N_CPUS)\n",
    "\n",
    "# Set username\n",
    "USER = os.environ[\"USER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29b983-7dcd-4cdc-9577-c870bd9b4169",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "952e8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Coqui-TTS parameters\n",
    "COPY_TO_SCRATCH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284b9f3d-cfc4-42b5-89c2-6009f44c3131",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# General params & paths\n",
    "run_name = \"test\"\n",
    "run_description = \"test\"\n",
    "project_name = \"TTS\"\n",
    "output_path = f\"/storage/plzen4-ntis/home/dcifka20/hds/tmp_files\"\n",
    "continue_path = \"\"\n",
    "restore_path = \"/storage/plzen4-ntis/projects/korpusy-public/vyuka/HDS2024/SP2/model.pth\"\n",
    "best_path = \"\"\n",
    "grad_accum_steps = 1     # Number of gradient accumulation steps. It is used to accumulate gradients over multiple batches (1)\n",
    "small_run = 100          # Number of samples to use (suitable for debugging) - defaults to None (all samples are used)\n",
    "coqui_path = f\"/storage/plzen4-ntis/home/dcifka20/GIT_repos/Coqui-TTS\"\n",
    "trainer_path = f\"/storage/plzen4-ntis/home/dcifka20/GIT_repos/Trainer\"\n",
    "\n",
    "# AUDIO PARAMS\n",
    "audio = {\n",
    "    # STFT params\n",
    "    \"fft_size\": 1024,     # number of stft frequency levels. Size of the linear spectogram frame.\n",
    "    \"win_length\": 1024,   # STFT window length\n",
    "    \"hop_length\": 256,    # STFT window hop-lengh\n",
    "    # Audio processing parameters\n",
    "    \"sample_rate\": 24000, # DATASET-RELATED: wav sample-rate.\n",
    "    # MelSpectrogram params\n",
    "    \"num_mels\": 80,       # size of the mel spec frame (80)\n",
    "    \"mel_fmin\": 0,        # DATASET-RELATED: minimum freq level for mel-spec (0). ~50 for male and ~95 for female voices.\n",
    "    \"mel_fmax\": 12000,    # DATASET-RELATED: maximum freq level for mel-spec (None)\n",
    "}\n",
    "\n",
    "# DATASET\n",
    "datasets = [   # List of datasets. They all merged and they get different speaker_ids.\n",
    "    {\"formatter\": \"artic\",\n",
    "     \"path\": f\"/storage/plzen4-ntis/home/dcifka20/hds/datasets/FulTo.cs-CZ.m\",\n",
    "     \"meta_file_train\": \"train.ph-redu.epa.csv\", # for vtck if list, ignore speakers id in list for train, its useful for test cloning with new speakers\n",
    "     \"ignored_speakers\": None,              # List of speakers IDs that are not used at the training (None).\n",
    "     \"language\": \"cs-cz\",                   # Language code of the dataset (None). If defined, it overrides `phoneme_language`.\n",
    "     \"meta_file_val\": None,                 # Name of the dataset meta file that defines the instances used at validation.\n",
    "     \"meta_file_attn_mask\": \"\",             # Path to the file that lists the attention mask files used with models that require attention masks to train the duration predictor.\n",
    "     # \"meta_file_dur\": meta_file_dur,        # duration of particular speech units (including punctuation, breaks etc.)\n",
    "    }\n",
    "]\n",
    "\n",
    "# VOCABULARY PARAMS\n",
    "characters = {     # Defines character or phoneme set used by the model\n",
    "    \"pad\": \"<PAD>\",    # characters in place of empty padding (None)\n",
    "    \"eos\": \"<EOS>\",    # characters showing the end of a sentence (None)\n",
    "    \"bos\": \"<BOS>\",    # characters showing the beginning of a sentence (None)\n",
    "    \"blank\": \"<BLNK>\", # Optional character used between characters by some models for better prosody.\n",
    "    # character set used by the model. Characters not in this list are ignored when converting input text to a list of sequence IDs (None).\n",
    "    # \"characters\": \"AÁÄBCČDĎEÉĚËFGHIÍJKLMNŇOÓÖPQRŘSŠTŤUÚŮÜVWXYÝZŽaáäbcčdďeéěëfghiíjklmnňoóöpqrřsštťuúůüvwxyýzž\",    # Czech graphemes\n",
    "    #\"characters\": \"0=abcdfijklmnoprstuvxzŋřɛɟɡɦɪɲʃʊʒʔː\", # Czech IPA\n",
    "    \"characters\": \"ACDEIJOPRSTUZabcdefghijklmnopqrstuvxz@#$*%Ç\",\n",
    "    # characters considered as punctuation as parsing the input sentence (None)\n",
    "    \"punctuations\": \"!,-.:;–/()?ˈ„“”\\\"‚‘’ˌː… \",\n",
    "    # characters considered as parsing phonemes (None)\n",
    "    # \"phonemes\": \"iyɨʉɯuɪʏʊeøɘəɵɤoɛœɜɞʌɔæɐaɶɑɒᵻʘɓǀɗǃʄǂɠǁʛpbtdʈɖcɟkɡqɢʔɴŋɲɳnɱmʙrʀⱱɾɽɸβfvθðszʃʒʂʐçʝxɣχʁħʕhɦɬɮʋɹɻjɰlɭʎʟˈˌːˑʍwɥʜʢʡɕʑɺɧɚ˞ɫ\"\n",
    "    \"phonemes\": None,\n",
    "}\n",
    "\n",
    "# VITS MODEL ARGS\n",
    "model_args = {\n",
    "    \"num_chars\": None,                          # Number of characters in the vocabulary (100)\n",
    "    \"out_channels\": 513,                        # Number of output channels (513)\n",
    "    \"spec_segment_size\": 32,                    # Decoder input segment size (32). `(32 * hoplength\": waveform length)`.\n",
    "    \"hidden_channels\": 192,                     # Number of hidden channels of the model (192)\n",
    "    \"hidden_channels_ffn_text_encoder\": 768,    # Number of hidden channels of the feed-forward layers of the text encoder transformer (768)\n",
    "    \"num_heads_text_encoder\": 2,                # Number of attention heads of the text encoder transformer (2)\n",
    "    \"num_layers_text_encoder\": 6,               # Number of transformer layers in the text encoder (6)\n",
    "    \"kernel_size_text_encoder\": 3,              # Kernel size of the text encoder transformer FFN layers (3)\n",
    "    \"dropout_p_text_encoder\": 0.1,              # Dropout rate of the text encoder (0.1)\n",
    "    \"dropout_p_duration_predictor\": 0.5,        # Dropout rate of the duration predictor (0.5)\n",
    "    \"kernel_size_posterior_encoder\": 5,         # Kernel size of the posterior encoder's WaveNet layers (5)\n",
    "    \"dilation_rate_posterior_encoder\": 1,       # Dilation rate of the posterior encoder's WaveNet layers (1)\n",
    "    \"num_layers_posterior_encoder\": 16,         # Number of posterior encoder's WaveNet layers (16)\n",
    "    \"kernel_size_flow\": 5,                      # Kernel size of the Residual Coupling layers of the flow network (5)\n",
    "    \"dilation_rate_flow\": 1,                    # Dilation rate of the Residual Coupling WaveNet layers of the flow network (1)     \n",
    "    \"num_layers_flow\": 4,                       # Number of Residual Coupling WaveNet layers of the flow network (4)\n",
    "    \"resblock_type_decoder\": \"2\",               # Type of the residual block in the decoder network (\"1\")\n",
    "    \"resblock_kernel_sizes_decoder\": [          # Kernel sizes of the residual blocks in the decoder network (`[3, 7, 11]`).\n",
    "        3, 5, 7\n",
    "    ],   \n",
    "    \"resblock_dilation_sizes_decoder\": [        # Dilation sizes of the residual blocks in the decoder network\n",
    "        [1, 2],                                 # (`[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`)\n",
    "        [2, 6],\n",
    "        [3, 12],\n",
    "    ],\n",
    "    \"upsample_rates_decoder\": [8, 8, 4],        # Upsampling rates for each concecutive upsampling layer in the decoder network.\n",
    "                                                # The multiply of these values must be equal to the kop length used for computing spectrograms\n",
    "                                                # (`[8, 8, 2, 2]`)\n",
    "    \"upsample_initial_channel_decoder\": 128,    # Number of hidden channels of the first upsampling convolution layer of the decoder network (512)\n",
    "    \"upsample_kernel_sizes_decoder\": [          # Kernel sizes for each upsampling layer of the decoder network (`[16, 16, 4, 4]`)\n",
    "        16, 16, 8\n",
    "    ],\n",
    "    \"periods_multi_period_discriminator\": [     # Periods values for Vits Multi-Period Discriminator (`[2, 3, 5, 7, 11]`)\n",
    "        2, 3, 5, 7, 11\n",
    "    ],\n",
    "    \"use_sdp\": False,                           # Use Stochastic Duration Predictor (True)\n",
    "    \"noise_scale\": 1.0,                         # Noise scale used for the sample noise tensor in training (1.0)\n",
    "    \"inference_noise_scale\": 0.667,             # Noise scale used for the sample noise tensor in inference (0.667)\n",
    "    \"length_scale\": 1,                          # Scale factor for the predicted duration values (1). Smaller values result faster speech.\n",
    "    \"noise_scale_dp\": 1.0,                      # Noise scale used by the Stochastic Duration Predictor sample noise in training (1.0)\n",
    "    \"inference_noise_scale_dp\": 0.8,            # Noise scale for the Stochastic Duration Predictor in inference (0.8)\n",
    "    \"max_inference_len\": None,                  # Maximum inference length to limit the memory use (None)\n",
    "    \"init_discriminator\": True,                 # Initialize the disciminator network if set True. Set False for inference.\n",
    "    \"use_spectral_norm_discriminator\": False,   # Use spectral normalization over weight norm in the discriminator (False)\n",
    "    \"detach_dp_input\": True,                    # Detach duration predictor's input from the network for stopping the gradients (True)\n",
    "    \"freeze_encoder\": False,                    # Freeze the encoder weigths during training (False)\n",
    "    \"freeze_DP\": False,                         # Freeze the duration predictor weigths during training (False)\n",
    "    \"freeze_PE\": False,                         # Freeze the posterior encoder weigths during training (False)\n",
    "    \"freeze_flow_decoder\": False,               # Freeze the flow encoder weigths during training (False)\n",
    "    \"freeze_waveform_decoder\": False,           # Freeze the waveform decoder weigths during training (False)\n",
    "    \"encoder_sample_rate\": None,                # If not None this sample rate will be used for training the Posterior Encoder, flow, text_encoder and duration predictor.\n",
    "                                                # The decoder part (vocoder) will be trained with the `config.audio.sample_rate` (None).\n",
    "    \"interpolate_z\": True,                      # If `encoder_sample_rate` not None and this parameter True the nearest interpolation will be used\n",
    "                                                # to upsampling the latent variable z with the sampling rate `encoder_sample_rate` to the `config.audio.sample_rate` (True).\n",
    "                                                # If it is False you will need to add extra `upsample_rates_decoder` to match the shape.\n",
    "    # MULTI-SPEAKER\n",
    "    \"num_speakers\": 0,                      # Number of speakers for the speaker embedding layer\n",
    "    \"use_speaker_embedding\": False,         # Enable/disable using speaker embeddings for multi-speaker models (False). If set True, the model is in the multi-speaker mode.\n",
    "    \"speakers_file\": None,                  # Path to the speaker mapping file for the Speaker Manager\n",
    "    \"speaker_embedding_channels\": 256,      # Number of speaker embedding channels (256)    \n",
    "    \"use_d_vector_file\": False,             # Enable/disable using external speaker embeddings in place of the learned embeddings (False)\n",
    "    \"d_vector_file\": None,                  # Path to the file including pre-computed speaker embeddings (None)\n",
    "    \"d_vector_dim\": None,                   # Channels of external speaker embedding vectors (0)\n",
    "    \"use_speaker_encoder_as_loss\": False,   # Enable/Disable Speaker Consistency Loss (SCL) (False)\n",
    "    \"speaker_encoder_config_path\": None,    # Path to the file speaker encoder config file, to use for SCL (\"\").\n",
    "    \"speaker_encoder_model_path\": None,     # Path to the file speaker encoder checkpoint file, to use for SCL (\"\").\n",
    "    \"condition_dp_on_speaker\": True,        # Condition the duration predictor on the speaker embedding (True)\n",
    "    # MULTI-LANGUAGE\n",
    "    \"use_language_embedding\": False,    # Enable/Disable language embedding for multilingual models (False)\n",
    "    \"embedded_language_dim\": 4,         # Number of language embedding channels (4)\n",
    "    \"num_languages\": 0,                 # Number of languages for the language embedding layer (0)\n",
    "    \"language_ids_file\": None,          # Path to the language mapping file for the Language Manager (None)\n",
    "}\n",
    "\n",
    "# CONFIG\n",
    "model_config = {\n",
    "    # DATA LOADING\n",
    "    \"num_loader_workers\": 1,               # number of training data loader processes. Don't set it too big. 4-8 are good values.\n",
    "    \"num_eval_loader_workers\": 1,          # number of evaluation data loader processes.\n",
    "    \"text_cleaner\": \"no_cleaners\",       # Name of the text cleaner used for cleaning and formatting transcripts.\n",
    "    \"enable_eos_bos_chars\": False,         # enable/disable beginning of sentence and end of sentence chars.\n",
    "    \"batch_group_size\": 5,                 # Size of the batch groups used for bucketing. By default, the dataloader orders samples by the sequence\n",
    "                                           # length for a more efficient and stable training. If `batch_group_size > 1` then it performs bucketing to\n",
    "                                           # prevent using the same batches for each epoch.\n",
    "    \"min_text_len\": 1,                     # Minimum length of input text to be used (0). All shorter samples will be ignored.\n",
    "    \"max_text_len\": 999,                   # Maximum length of input text to be used (float(\"inf\")). All longer samples will be ignored.\n",
    "    \"min_audio_len\": 7200,                 # Minimum length of input audio to be used (0). All shorter samples will be ignored.\n",
    "    \"max_audio_len\": 480000,               # Maximum length of input audio to be used (float(\"inf\")). All longer samples will be ignored.\n",
    "                                           # The maximum length in the dataset defines the VRAM used in the training.\n",
    "                                           # Hence, pay attention to this value if you encounter an OOM error in training.\n",
    "                                           # For FS=24kHz and max audio length 15s: # 360000 = 24000 * 15\n",
    "    \"start_by_longest\": True,              # Start by longest sequence. It is especially useful to check OOM (False)\n",
    "    \"compute_input_seq_cache\": True,       # If true, text sequences are computed before starting training. If phonemes are enabled, they are also computed at this stage.\n",
    "    \"use_noise_augment\": False,            # Augment the input audio with random noise\n",
    "    \"add_blank\": False,                    # Add blank characters between each other two characters (True). It improves performance for some models at expense of slower run-time due to the longer input sequence.\n",
    "    \"compute_linear_spec\": True,           # If True, the linear spectrogram is computed and returned alongside the mel output (True). Do not change.\n",
    "    \"return_wav\": True,                    # If true, data loader returns the waveform as well as the other outputs (True). Do not change.\n",
    "    \"compute_f0\": False,\n",
    "\n",
    "    # PHONEMES\n",
    "    \"phoneme_cache_path\": \"phoneme_cache\", # phoneme computation is slow, therefore, it caches results in the given folder\n",
    "    \"use_phonemes\": False,                 # use phonemes instead of raw characters. It is suggested for better pronounciation.\n",
    "    \"phoneme_language\": \"cz-cz\",           # depending on your target language, pick one from  https\"://github.com/bootphon/phonemizer#languages\n",
    "\n",
    "    # DISTRIBUTED TRAINING\n",
    "    \"distributed_backend\": \"gloo\",\n",
    "    \"distributed_url\": \"tcp://localhost:54321\",\n",
    "\n",
    "    # TRAINING\n",
    "    \"epochs\": 10,              # total number of epochs to train (10000)\n",
    "    \"use_total_epochs\": True,  # JMa: Compute the number of epochs done as a total number across continue runs (False). If True, total number of epochs is added to checkpoint path.\n",
    "    \"stop_after_steps\": False, # JMa: Stop training after defined step (False)\n",
    "    \"steps\": 1000000,          # JMa: \"Number of steps to stop training when `stop_after_steps` is True (1000000).\n",
    "    \"batch_size\": 16,          # Batch size for training. Lower values than 32 might cause hard to learn attention.\n",
    "    \"mixed_precision\": True,   # level of optimization with NVIDIA's apex feature for automatic mixed FP16/FP32 precision (AMP), NOTE: currently only O1 is supported, and use \"O1\" to activate.\n",
    "    \"loss_masking\": None,\n",
    "\n",
    "    # VALIDATION\n",
    "    \"run_eval\": True,               # Run evaluation after each epoch.\n",
    "    \"eval_batch_size\": 16,          # Validation batch size.\n",
    "    \"eval_split_max_size\": 256,     # Number maximum of samples to be used for evaluation in proportion split. Defaults to None (Disabled).\n",
    "    \"eval_split_size\": 0.01,        # If between 0.0 and 1.0 represents the proportion of the dataset to include in the evaluation set. \n",
    "                                    # If > 1, represents the absolute number of evaluation samples. Defaults to 0.01 (1%).\n",
    "    \"test_delay_epochs\": -1,        # Until attention is aligned, testing only wastes computation time.\n",
    "    \"test_epoch_step\": 1,           # JMa: Number of epochs to run test and generate testing files (1)\n",
    "    \"save_test_files\": True,        # JMa: Save test files (False)\n",
    "    \"test_sentences_file\": None,    # set a file to load sentences to be used for testing. If it is null then we use default english sentences.\n",
    "    \"test_sentences\": [             # sentences to be used for testing\"\n",
    "        [\"$ pRIliZ ZluTouCkI kUJ Upjel DAbelskE Odi. $\"],\n",
    "        [\"$ strC prst skrs krk, # nebo Ti ho tam strCIm sAm. $\"],\n",
    "        [\"$ tohle je pokus, # snat to vijde. $\"],\n",
    "        [\"$ omlouvAm se, # tohle se stAvA maksimAlJe jednou za deset let. $\"],\n",
    "        [\"$ tRista tRicet tRi stRIbrnIx stRIkaCek stRIkalo pRes tRista tRicet tRi stRIbrnIx stRex. $\"],\n",
    "        [\"$ pRAl bix si, # abix bil uZ doma. $\"],\n",
    "        [\"$ pUjdeme zItra do kina? $\"],\n",
    "        [\"$ proC to DelAS? $\"],\n",
    "    ],\n",
    "    # OPTIMIZER\n",
    "    \"lr\": 0.001,                           # Learning rate for each optimizer (0.001)\n",
    "    \"lr_scheduler\": None,                  # Learning rate scheduler(s) to use (None)\n",
    "    \"lr_scheduler_params\": None,           # Learning rate scheduler(s) arguments (None)\n",
    "    \"optimizer\": \"AdamW\",                  # Optimizer used for the training.\n",
    "    \"optimizer_params\": {                  # Optimizer kwargs.\n",
    "        \"betas\": [0.8, 0.99],\n",
    "        \"eps\": 0.000000001,\n",
    "        \"weight_decay\": 0.01,              # Weight decay weight.\n",
    "    },\n",
    "    \"use_grad_scaler\": False,              # Enable/disable gradient scaler explicitly. It is enabled by default with AMP training (False)\n",
    "    \"lr_gen\": 0.0002,                      # Initial learning rate for the generator (0.0002)\n",
    "    \"lr_disc\": 0.0002,                     # Initial learning rate for the discriminator (0.0002)\n",
    "    \"lr_scheduler_gen\": \"ExponentialLR\",   # Name of the learning rate scheduler for the generator. One of the `torch.optim.lr_scheduler.*` (`ExponentialLR`).\n",
    "    \"lr_scheduler_gen_params\": {           # Parameters for the learning rate scheduler of the generator. Defaults to `{'gamma'\": 0.999875, \"last_epoch\":-1}`.\n",
    "        \"gamma\": 0.999875,\n",
    "        \"last_epoch\": -1,\n",
    "    },\n",
    "    \"lr_scheduler_disc\": \"ExponentialLR\",  # Name of the learning rate scheduler for the discriminator. One of the `torch.optim.lr_scheduler.*` (`ExponentialLR`).\n",
    "    \"lr_scheduler_disc_params\": {          # Parameters for the learning rate scheduler of the generator. Defaults to `{'gamma'\": 0.999875, \"last_epoch\":-1}`.\n",
    "        \"gamma\": 0.999875,\n",
    "        \"last_epoch\": -1,\n",
    "    },\n",
    "    \"grad_clip\":  [1000, 1000],            # Gradient clipping thresholds for each optimizer\n",
    "    \"scheduler_after_epoch\": True,         # If true, step the scheduler after each epoch else after each step (True).\n",
    "\n",
    "    # LOSS PARAMS\n",
    "    \"kl_loss_alpha\": 5.0,               # Loss weight for KL loss (1.0)\n",
    "    \"disc_loss_alpha\": 1.0,             # Loss weight for the discriminator loss (1.0)\n",
    "    \"gen_loss_alpha\": 1.0,              # Loss weight for the generator loss (1.0)\n",
    "    \"feat_loss_alpha\": 1.0,             # Loss weight for the feature matching loss (1.0)\n",
    "    \"mel_loss_alpha\": 45.0,             # Loss weight for the mel loss (45.0)\n",
    "    \"dur_loss_alpha\": 1.0,              # Loss weight for duration loss (1.0)\n",
    "    \"speaker_encoder_loss_alpha\": 9.0,  # Speaker Consistency Loss (SCL) α to 9 like in the YourTTS paper\n",
    "                                        # (used when `use_speaker_encoder_as_loss = True`)\n",
    "\n",
    "    # SAMPLE BALANCING\n",
    "    \"use_speaker_weighted_sampler\": False,  # Enable/Disable the batch balancer by speaker (False).\n",
    "    \"speaker_weighted_sampler_alpha\": 1.0,  # Number that control the influence of the speaker sampler weights (1.0)\n",
    "    \"use_language_weighted_sampler\": False, # Enable/Disable the batch balancer by language (False)\n",
    "    \"language_weighted_sampler_alpha\": 1.0, # Number that control the influence of the language sampler weights (1.0)\n",
    "    \"use_length_weighted_sampler\": False,   # Enable/Disable the batch balancer by audio length (False). If enabled the dataset will be divided into 10 buckets\n",
    "                                            # considering the min and max audio of the dataset. The sampler weights will be computed forcing to have\n",
    "                                            # the same quantity of data for each bucket in each training batch.\n",
    "    \"length_weighted_sampler_alpha\": 1.0,   # Number that control the influence of the length sampler weights (1.0)\n",
    "    \"use_weighted_sampler\": False,          # If true, use weighted sampler with bucketing for balancing samples between datasets used in training (False)\n",
    "    \"weighted_sampler_attrs\": {             # Key returned by the formatter to be used for weighted sampler ({}).\n",
    "        \"speaker_name\": 1.0,                # For example `{\"root_path\": 2.0, \"speaker_name\": 1.0}` sets sample probabilities\n",
    "    },\n",
    "                                            # by overweighting `root_path` by 2.0.\n",
    "    \"weighted_sampler_multipliers\": {},     # Weight each unique value of a key returned by the formatter for weighted sampling ({}).\n",
    "                                            # For example `{\"root_path\":{\"/raid/datasets/libritts-clean-16khz-bwe-coqui_44khz/LibriTTS/train-clean-100/\": 1.0,\n",
    "                                            #                            \"/raid/datasets/libritts-clean-16khz-bwe-coqui_44khz/LibriTTS/train-clean-360/\": 0.5}`.\n",
    "                                            # It will sample instances from `train-clean-100` 2 times more than `train-clean-360`.\n",
    "\n",
    "    # TENSORBOARD, LOGGING & CHECKPOINTING\n",
    "    \"print_step\": 25,                  # Number of steps to log training on console.\n",
    "    \"plot_step\": 25,                   # Number of steps required to print the next training log.\n",
    "    \"dashboard_logger\": \"tensorboard\", # \"tensorboard\" or \"wandb\"\n",
    "    \"print_eval\": True,                # If True, it prints intermediate loss values in evalulation\n",
    "    \"save_checkpoints\": True,          # If true, it saves checkpoints per \"save_step\"\n",
    "    \"save_on_epochs\": True,            # JMa: If True, checkpoints/models are saved based on epochs (False)\n",
    "    \"save_step\": 5000,                 # Number of training steps expected to save training stats and checkpoints (10000)\n",
    "    \"save_epoch\": 10,                  # Number of training epochs expected to save training stats and checkpoints (25). Used instead of `save_steps` when `use_total_epochs == True`.\n",
    "    \"log_model_step\": None,            # Save checkpoint to the logger every `log_model_step`` steps (None). If not defined `log_model_step == save_step`.\n",
    "    \"log_model_epoch\": None,           # Save checkpoint to the logger every `log_model_epoch`` epochs (None). If not defined `log_model_epoch == save_epoch`. Used instead of `log_model_step` when `use_total_epochs == True`.\n",
    "    \"save_n_checkpoints\": 2,           # Keep n local checkpoints (5).\n",
    "    \"save_all_best\": False,            # If true, save all best checkpoints and keep the older ones.\n",
    "    \"save_best_after\": 10000,          # Global step after which to save best models if save_all_best is true (10000)\n",
    "    \"model_param_stats\": False,        # Enable/Disable logging internal model stats for model diagnostic. It might be useful for model debugging. Defaults to False.\n",
    "    \"log_test_files\": False,           # JMa: Log test files (True)\n",
    "    \"use_epoch_in_path\": True          # JMa: If True, total number of epochs is added to checkpoint/model and test file path (False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b8b0f2-629c-4d7f-be92-18ab310a4015",
   "metadata": {
    "tags": [
     "injected_parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Non-Coqui-TTS parameters\n",
    "COPY_TO_SCRATCH = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65273fdc-b422-47d6-8358-f057a0a1a8a0",
   "metadata": {},
   "source": [
    "# Copy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075d5828-e730-4bd8-8ae3-9f6b5ba3f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Copying data to local scratch: /scratch.ssd/dcifka20/job_1688648.pbs-m1.metacentrum.cz/FulTo.cs-CZ.m\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/scratch.ssd/dcifka20/job_1688648.pbs-m1.metacentrum.cz/FulTo.cs-CZ.m'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Copy dataset to local scratch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Copying data to local scratch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_scratch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_scratch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Store the scratch dataset so that it is used for training\u001b[39;00m\n\u001b[1;32m     11\u001b[0m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_scratch\n",
      "File \u001b[0;32m/usr/lib/python3.10/shutil.py:559\u001b[0m, in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(src) \u001b[38;5;28;01mas\u001b[39;00m itr:\n\u001b[1;32m    558\u001b[0m     entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itr)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_copytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mignore_dangling_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_dangling_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/shutil.py:457\u001b[0m, in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     ignored_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 457\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    459\u001b[0m use_srcentry \u001b[38;5;241m=\u001b[39m copy_function \u001b[38;5;129;01mis\u001b[39;00m copy2 \u001b[38;5;129;01mor\u001b[39;00m copy_function \u001b[38;5;129;01mis\u001b[39;00m copy\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/scratch.ssd/dcifka20/job_1688648.pbs-m1.metacentrum.cz/FulTo.cs-CZ.m'"
     ]
    }
   ],
   "source": [
    "if COPY_TO_SCRATCH:\n",
    "    # Copy datasets\n",
    "    DATASETS_SCRATCH = []\n",
    "    for d in datasets:\n",
    "        # Prepare dataset dir in the scratch\n",
    "        dataset_scratch = os.path.join(os.environ[\"SCRATCHDIR\"], os.path.basename(d[\"path\"]))\n",
    "        # Copy dataset to local scratch\n",
    "        print(f\"> Copying data to local scratch: {dataset_scratch}\")\n",
    "        shutil.copytree(d[\"path\"], dataset_scratch)\n",
    "        # Store the scratch dataset so that it is used for training\n",
    "        d[\"path\"] = dataset_scratch\n",
    "        # Store paths to individual datasets in scratch so that they can be deleted in the end\n",
    "        DATASETS_SCRATCH.append(dataset_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a82a2f",
   "metadata": {},
   "source": [
    "# Set path to training framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a6ca9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to (modified) Coqui-TTS\n",
    "sys.path.insert(0, coqui_path)\n",
    "# Set path to (modified) Coqui-Trainer\n",
    "sys.path.insert(0, trainer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b88a2-3ab7-45a1-a0e9-d5cd98ed226c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b746fe4-9f72-43a6-8bd0-da5fd7200a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Python & module versions...\n",
      " | > Python:    3.10.12\n",
      " | > PyTorch:   2.1.2+cu121\n",
      " | > Coqui-TTS: 0.22.0\n",
      " | > Trainer:   v0.0.36\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:24000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:12000\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > ARTIC dataset: voice=FulTo, sex=m, language=cs-CZ\n",
      " | > Found 1000 files in /auto/plzen4-ntis/home/dcifka20/hds/datasets/FulTo.cs-CZ.m\n",
      " | > # training files:      990\n",
      " | > # evaluation files:    10\n",
      " | > evaluation split size: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /auto/plzen4-ntis)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: True\n",
      " | > Precision: fp16\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 64\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      "2024-05-10 13:41:34.141236: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-10 13:41:34.141307: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-10 13:41:34.141336: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-10 13:41:34.151456: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      " > Start Tensorboard: tensorboard --logdir=/storage/plzen4-ntis/home/dcifka20/hds/tmp_files/test\n",
      "[!] Small Run, only using 100 samples.\n",
      " > Restoring from model.pth ...\n",
      " > Restoring Model...\n",
      " > Partial model initialization...\n",
      " | > Layer missing in the model definition: emb_g.weight\n",
      " | > Layer missing in the model definition: posterior_encoder.enc.cond_layer.bias\n",
      " | > Layer missing in the model definition: posterior_encoder.enc.cond_layer.parametrizations.weight.original0\n",
      " | > Layer missing in the model definition: posterior_encoder.enc.cond_layer.parametrizations.weight.original1\n",
      " | > Layer missing in the model definition: flow.flows.0.enc.cond_layer.bias\n",
      " | > Layer missing in the model definition: flow.flows.0.enc.cond_layer.parametrizations.weight.original0\n",
      " | > Layer missing in the model definition: flow.flows.0.enc.cond_layer.parametrizations.weight.original1\n",
      " | > Layer missing in the model definition: flow.flows.1.enc.cond_layer.bias\n",
      " | > Layer missing in the model definition: flow.flows.1.enc.cond_layer.parametrizations.weight.original0\n",
      " | > Layer missing in the model definition: flow.flows.1.enc.cond_layer.parametrizations.weight.original1\n",
      " | > Layer missing in the model definition: flow.flows.2.enc.cond_layer.bias\n",
      " | > Layer missing in the model definition: flow.flows.2.enc.cond_layer.parametrizations.weight.original0\n",
      " | > Layer missing in the model definition: flow.flows.2.enc.cond_layer.parametrizations.weight.original1\n",
      " | > Layer missing in the model definition: flow.flows.3.enc.cond_layer.bias\n",
      " | > Layer missing in the model definition: flow.flows.3.enc.cond_layer.parametrizations.weight.original0\n",
      " | > Layer missing in the model definition: flow.flows.3.enc.cond_layer.parametrizations.weight.original1\n",
      " | > Layer missing in the model definition: duration_predictor.cond.weight\n",
      " | > Layer missing in the model definition: duration_predictor.cond.bias\n",
      " | > Layer missing in the model definition: waveform_decoder.cond_layer.weight\n",
      " | > Layer missing in the model definition: waveform_decoder.cond_layer.bias\n",
      " | > Layer dimention missmatch between model definition and checkpoint: text_encoder.emb.weight\n",
      " | > 509 / 510 layers are restored.\n",
      " > Model restored from step 61000 (epoch 999)\n",
      "\n",
      " > Model has 68242301 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/10\u001b[0m\n",
      " --> /storage/plzen4-ntis/home/dcifka20/hds/tmp_files/test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Training arguments...\n",
      " | > gradient accumulation steps: 1\n",
      " | > true batch size: 16\n",
      " | > learning rate: 0.001\n",
      " | > save test files: True (each 1 epochs)\n",
      " | > log test files: False\n",
      " | > use total epochs: True\n",
      " | > checkpoint model: 10 epochs\n",
      " | > stop training: 10 epochs\n",
      "\n",
      "\n",
      "> DataLoader initialization\n",
      "| > Tokenizer:\n",
      "\t| > add_blank: False\n",
      "\t| > use_eos_bos: False\n",
      "\t| > use_phonemes: False\n",
      "| > Number of instances : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > TRAINING (2024-05-10 13:41:39) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Preprocessing samples\n",
      " | > Max text length: 145\n",
      " | > Min text length: 47\n",
      " | > Avg text length: 84.91\n",
      " | \n",
      " | > Max audio length: 312000\n",
      " | > Min audio length: 117600\n",
      " | > Avg audio length: 192455.88\n",
      " | > Num. instances discarded samples: 0\n",
      " | > Batch group size: 80.\n",
      "$ ve zvjeTe se fSak podle mlufCI eurotelu ivi taTounovE v posledJI dobje objevila dalSI noviNka. $\n",
      " [!] Character 'N' not found in the vocabulary. Discarding it.\n",
      "$ jiRI jirous pozastavI zvUj pRedimenzovanI vulgarostroj, # a pRemItA o hodnotAdzh po~ezije a jImavje recituje spatra. $\n",
      " [!] Character '~' not found in the vocabulary. Discarding it.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "$ sedmaosmdesATiletI bAsJIk ji zIskal za zvou posledJI kJihu nazvanou po^uliCJI psIk. $\n",
      " [!] Character '^' not found in the vocabulary. Discarding it.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> DataLoader initialization\n",
      "| > Tokenizer:\n",
      "\t| > add_blank: False\n",
      "\t| > use_eos_bos: False\n",
      "\t| > use_phonemes: False\n",
      "| > Number of instances : 10\n",
      " | > Preprocessing samples\n",
      " | > Max text length: 121\n",
      " | > Min text length: 56\n",
      " | > Avg text length: 95.2\n",
      " | \n",
      " | > Max audio length: 300000\n",
      " | > Min audio length: 132000\n",
      " | > Avg audio length: 221039.6\n",
      " | > Num. instances discarded samples: 0\n",
      " | > Batch group size: 0.\n",
      "$ podle andreje staNkoviCe je lukeSUv urAZlivI ClAnek pouhou metaforou, # vijadRujIcI rozhoRCeJI nad dzhaosem. $\n",
      " [!] Character 'N' not found in the vocabulary. Discarding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ podle andreje staNkoviCe je lukeSUv urAZlivI ClAnek pouhou metaforou, # vijadRujIcI rozhoRCeJI nad dzhaosem. $\n",
      " [!] Character 'N' not found in the vocabulary. Discarding it.\n",
      "Volám diskriminátor pro výpočet loss funkce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_disc: 1.3587108850479126  (1.3587108850479126)\n",
      "     | > loss_disc_real_0: 0.12047269195318222  (0.12047269195318222)\n",
      "     | > loss_disc_real_1: 0.1178659126162529  (0.1178659126162529)\n",
      "     | > loss_disc_real_2: 0.20523667335510254  (0.20523667335510254)\n",
      "     | > loss_disc_real_3: 0.11560378223657608  (0.11560378223657608)\n",
      "     | > loss_disc_real_4: 0.15837617218494415  (0.15837617218494415)\n",
      "     | > loss_disc_real_5: 0.09131519496440887  (0.09131519496440887)\n",
      "     | > loss_0: 1.3587108850479126  (1.3587108850479126)\n",
      "     | > loss_gen: 3.8523104190826416  (3.8523104190826416)\n",
      "     | > loss_kl: 22.580995559692383  (22.580995559692383)\n",
      "     | > loss_feat: 71.39514923095703  (71.39514923095703)\n",
      "     | > loss_mel: 38.266658782958984  (38.266658782958984)\n",
      "     | > loss_1: 136.09510803222656  (136.09510803222656)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volám generátor pro výpočet loss funkcí.\n",
      " | > Synthesizing test sentences.\n",
      " | > Saving 8 test audio files at step/epoch 000061008-00000\n",
      " | > Saving 8 test figures at step/epoch 000061008-00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time: 0.3341529369354248 \u001b[0m(+0)\n",
      "     | > avg_loss_disc: 1.3587108850479126 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_0: 0.12047269195318222 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_1: 0.1178659126162529 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_2: 0.20523667335510254 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_3: 0.11560378223657608 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_4: 0.15837617218494415 \u001b[0m(+0)\n",
      "     | > avg_loss_disc_real_5: 0.09131519496440887 \u001b[0m(+0)\n",
      "     | > avg_loss_0: 1.3587108850479126 \u001b[0m(+0)\n",
      "     | > avg_loss_gen: 3.8523104190826416 \u001b[0m(+0)\n",
      "     | > avg_loss_kl: 22.580995559692383 \u001b[0m(+0)\n",
      "     | > avg_loss_feat: 71.39514923095703 \u001b[0m(+0)\n",
      "     | > avg_loss_mel: 38.266658782958984 \u001b[0m(+0)\n",
      "     | > avg_loss_1: 136.09510803222656 \u001b[0m(+0)\n",
      "\n",
      " > BEST MODEL : /storage/plzen4-ntis/home/dcifka20/hds/tmp_files/test/best_model_61008-0.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 1/10\u001b[0m\n",
      " --> /storage/plzen4-ntis/home/dcifka20/hds/tmp_files/test\n",
      "\n",
      "\u001b[1m > TRAINING (2024-05-10 13:42:14) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ ve zvjeTe se fSak podle mlufCI eurotelu ivi taTounovE v posledJI dobje objevila dalSI noviNka. $\n",
      " [!] Character 'N' not found in the vocabulary. Discarding it.\n",
      "$ jiRI jirous pozastavI zvUj pRedimenzovanI vulgarostroj, # a pRemItA o hodnotAdzh po~ezije a jImavje recituje spatra. $\n",
      " [!] Character '~' not found in the vocabulary. Discarding it.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "$ sedmaosmdesATiletI bAsJIk ji zIskal za zvou posledJI kJihu nazvanou po^uliCJI psIk. $\n",
      " [!] Character '^' not found in the vocabulary. Discarding it.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n",
      "$ podle andreje staNkoviCe je lukeSUv urAZlivI ClAnek pouhou metaforou, # vijadRujIcI rozhoRCeJI nad dzhaosem. $\n",
      " [!] Character 'N' not found in the vocabulary. Discarding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ podle andreje staNkoviCe je lukeSUv urAZlivI ClAnek pouhou metaforou, # vijadRujIcI rozhoRCeJI nad dzhaosem. $\n",
      " [!] Character 'N' not found in the vocabulary. Discarding it.\n",
      "Volám diskriminátor pro výpočet loss funkce\n",
      "Volám generátor pro výpočet loss funkcí.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_disc: 1.5713304281234741  (1.5713304281234741)\n",
      "     | > loss_disc_real_0: 0.11404550075531006  (0.11404550075531006)\n",
      "     | > loss_disc_real_1: 0.162921741604805  (0.162921741604805)\n",
      "     | > loss_disc_real_2: 0.3405941128730774  (0.3405941128730774)\n",
      "     | > loss_disc_real_3: 0.16517972946166992  (0.16517972946166992)\n",
      "     | > loss_disc_real_4: 0.1545741707086563  (0.1545741707086563)\n",
      "     | > loss_disc_real_5: 0.20709426701068878  (0.20709426701068878)\n",
      "     | > loss_0: 1.5713304281234741  (1.5713304281234741)\n",
      "     | > loss_gen: 4.681225299835205  (4.681225299835205)\n",
      "     | > loss_kl: 7.366035461425781  (7.366035461425781)\n",
      "     | > loss_feat: 92.36017608642578  (92.36017608642578)\n",
      "     | > loss_mel: 28.425506591796875  (28.425506591796875)\n",
      "     | > loss_1: 132.83294677734375  (132.83294677734375)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n",
      " | > Saving 8 test audio files at step/epoch 000061015-00001\n",
      " | > Saving 8 test figures at step/epoch 000061015-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.206404447555542 \u001b[0m(-0.1277484893798828)\n",
      "     | > avg_loss_disc:\u001b[91m 1.5713304281234741 \u001b[0m(+0.21261954307556152)\n",
      "     | > avg_loss_disc_real_0:\u001b[92m 0.11404550075531006 \u001b[0m(-0.006427191197872162)\n",
      "     | > avg_loss_disc_real_1:\u001b[91m 0.162921741604805 \u001b[0m(+0.045055828988552094)\n",
      "     | > avg_loss_disc_real_2:\u001b[91m 0.3405941128730774 \u001b[0m(+0.13535743951797485)\n",
      "     | > avg_loss_disc_real_3:\u001b[91m 0.16517972946166992 \u001b[0m(+0.04957594722509384)\n",
      "     | > avg_loss_disc_real_4:\u001b[92m 0.1545741707086563 \u001b[0m(-0.003802001476287842)\n",
      "     | > avg_loss_disc_real_5:\u001b[91m 0.20709426701068878 \u001b[0m(+0.11577907204627991)\n",
      "     | > avg_loss_0:\u001b[91m 1.5713304281234741 \u001b[0m(+0.21261954307556152)\n",
      "     | > avg_loss_gen:\u001b[91m 4.681225299835205 \u001b[0m(+0.8289148807525635)\n",
      "     | > avg_loss_kl:\u001b[92m 7.366035461425781 \u001b[0m(-15.214960098266602)\n",
      "     | > avg_loss_feat:\u001b[91m 92.36017608642578 \u001b[0m(+20.96502685546875)\n",
      "     | > avg_loss_mel:\u001b[92m 28.425506591796875 \u001b[0m(-9.84115219116211)\n",
      "     | > avg_loss_1:\u001b[92m 132.83294677734375 \u001b[0m(-3.2621612548828125)\n",
      "\n",
      " > BEST MODEL : /storage/plzen4-ntis/home/dcifka20/hds/tmp_files/test/best_model_61015-1.pth\n",
      " > Keyboard interrupt detected.\n",
      " > Saving model before exiting...\n",
      "\n",
      " > CHECKPOINT : /storage/plzen4-ntis/home/dcifka20/hds/tmp_files/test/checkpoint_61015-1.pth\n",
      " ! Run is kept in /storage/plzen4-ntis/home/dcifka20/hds/tmp_files/test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Trainer: Where the ✨️ happens.\n",
    "# TrainerArgs: Defines the set of arguments of the Trainer.\n",
    "from trainer import Trainer, TrainerArgs\n",
    "# VitsConfig: all model related values for training, validating and testing.\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "from TTS.tts.models.vits import VitsArgs, VitsAudioConfig\n",
    "from TTS.hds.vits import VitsHDS\n",
    "# BaseDatasetConfig: defines name, formatter and path of the dataset\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "# TTSTokenizer: defines tokens\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "# CharactersConfig: defines characters/phonemes\n",
    "from TTS.tts.configs.shared_configs import CharactersConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.utils.languages import LanguageManager\n",
    "# To check module version\n",
    "from TTS import __version__ as coqui_tts_version\n",
    "from trainer import __version__ as trainer_version\n",
    "from torch import __version__ as torch_version\n",
    "from platform import python_version\n",
    "\n",
    "print(\" > Python & module versions...\")\n",
    "print(f\" | > Python:    {python_version()}\")\n",
    "print(f\" | > PyTorch:   {torch_version}\")\n",
    "print(f\" | > Coqui-TTS: {coqui_tts_version}\")\n",
    "print(f\" | > Trainer:   {trainer_version}\")\n",
    "\n",
    "# Set audio config\n",
    "audio_config = VitsAudioConfig(**audio)\n",
    "# Set dataset config\n",
    "dataset_config = [BaseDatasetConfig(**d) for d in datasets]\n",
    "# Set characters config\n",
    "character_config = CharactersConfig(**characters)\n",
    "\n",
    "# VITS model args\n",
    "vits_args = VitsArgs(**model_args)\n",
    "\n",
    "# VITS config\n",
    "config = VitsConfig(\n",
    "    # General params and paths\n",
    "    run_name=run_name,\n",
    "    run_description=run_description,\n",
    "    project_name=project_name,\n",
    "    output_path=output_path,\n",
    "    # Model args\n",
    "    model_args=vits_args,\n",
    "    # Audio config\n",
    "    audio=audio_config,\n",
    "    # Datasets config\n",
    "    datasets=dataset_config,\n",
    "    # Character config\n",
    "    characters=character_config,\n",
    "    **model_config\n",
    ")\n",
    "\n",
    "# INITIALIZE THE AUDIO PROCESSOR\n",
    "# Audio processor is used for feature extraction and audio I/O.\n",
    "# It mainly serves to the dataloader and the training loggers.\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "\n",
    "# INITIALIZE THE TOKENIZER\n",
    "# Tokenizer is used to convert text to sequences of token IDs.\n",
    "# config is updated with the default characters if not defined in the config.\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "# LOAD DATA SAMPLES\n",
    "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
    "# You can define your custom sample loader returning the list of samples.\n",
    "# Or define your custom formatter and pass it to the `load_tts_samples`.\n",
    "# Check `TTS.tts.datasets.load_tts_samples` for more details.\n",
    "train_samples, eval_samples = load_tts_samples(dataset_config,\n",
    "                                               eval_split=True,\n",
    "                                               eval_split_max_size=config.eval_split_max_size,\n",
    "                                               eval_split_size=config.eval_split_size)\n",
    "print(f\" | > # training files:      {len(train_samples)}\")\n",
    "print(f\" | > # evaluation files:    {len(eval_samples)}\")\n",
    "print(f\" | > evaluation split size: {config.eval_split_size}\")\n",
    "\n",
    "# INITIALIZE THE MODEL\n",
    "# Models take a config object and a speaker manager as input\n",
    "# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n",
    "# Speaker manager is used by multi-speaker models.\n",
    "model = VitsHDS(config, ap, tokenizer)\n",
    "\n",
    "# INITIALIZE THE TRAINER\n",
    "# Trainer provides a generic API to train all the 🐸TTS models with all its perks like mixed-precision training,\n",
    "# distributed training, etc.\n",
    "# Trainer arguments\n",
    "trainer_args = {\n",
    "    \"continue_path\": continue_path,\n",
    "    \"restore_path\": restore_path,\n",
    "    \"best_path\": best_path,\n",
    "    \"use_unique_model_folder\": False,\n",
    "    \"grad_accum_steps\": grad_accum_steps,\n",
    "    \"small_run\": small_run,\n",
    "}\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(**trainer_args),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")\n",
    "\n",
    "print(\" > Training arguments...\")\n",
    "print(f\" | > gradient accumulation steps: {grad_accum_steps}\")\n",
    "print(f\" | > true batch size: {int(grad_accum_steps*int(config.batch_size))}\")\n",
    "print(f\" | > learning rate: {config.lr}\")\n",
    "print(f\" | > save test files: {config.save_test_files} (each {config.test_epoch_step} epochs)\")\n",
    "print(f\" | > log test files: {config.log_test_files}\")\n",
    "print(f\" | > use total epochs: {config.use_total_epochs}\")\n",
    "if config.save_on_epochs:\n",
    "    print(f\" | > checkpoint model: {config.save_epoch} epochs\")\n",
    "else:\n",
    "    print(f\" | > checkpoint model: {config.save_step} steps\")\n",
    "if config.stop_after_steps:\n",
    "    print(f\" | > stop training: {config.steps} steps\")\n",
    "else:\n",
    "    print(f\" | > stop training: {config.epochs} epochs\")\n",
    "\n",
    "# AND... 3,2,1... 🚀\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddb788-d61f-4dfe-acc9-66c6546feb3f",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a8a1c-59ee-41e0-8d15-bcf7dbe905a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COPY_TO_SCRATCH:\n",
    "    for d in DATASETS_SCRATCH:\n",
    "        # Delete local dataset directory\n",
    "        shutil.rmtree(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29bba4-7c53-4dcc-9d66-4f02929f3d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
